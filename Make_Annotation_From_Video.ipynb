{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joint Annotation 만드는 ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAEKYU\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Pose Class Loading Finish\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import network as net\n",
    "import json\n",
    "from Training import openpose\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import maximum_filter\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "import ast\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "            np.int16, np.int32, np.int64, np.uint8,\n",
    "            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
    "            np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_batch(img_path, batch_size):\n",
    "    num_of_data = len(img_path)\n",
    "    index = np.arange(0, num_of_data)\n",
    "    np.random.shuffle(index)\n",
    "    index = index[:batch_size]\n",
    "    \n",
    "    shuffled_img_data = [img_path[i] for i in index]\n",
    "    \n",
    "    return np.asarray(shuffled_img_data)\n",
    "\n",
    "def path_to_image(img_path, batch_size):\n",
    "    #buffer 선언\n",
    "    image_data = np.zeros((batch_size, 356, 356, 3), np.uint8)\n",
    "    \n",
    "    index = 0\n",
    "    for img in (img_path):\n",
    "        image_data[index] = cv2.imread(img)\n",
    "        index = index + 1;\n",
    "\n",
    "        \n",
    "        \n",
    "    return image_data\n",
    "\n",
    "def load_test_data(save_path):\n",
    "    img_path = save_path#\"./MPII_Dataset/resized_test_image/\"\n",
    "    file_path = []\n",
    "    file_list = os.listdir(img_path)\n",
    "    for i in (file_list):\n",
    "        file_path.append(img_path + i)\n",
    "    return file_path\n",
    "\n",
    "def padRightDownCorner(img, stride, padValue):\n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "\n",
    "    pad = 4 * [None]\n",
    "    pad[0] = 0 # up\n",
    "    pad[1] = 0 # left\n",
    "    pad[2] = 0 if (h%stride==0) else stride - (h % stride) # down\n",
    "    pad[3] = 0 if (w%stride==0) else stride - (w % stride) # right\n",
    "\n",
    "    img_padded = img\n",
    "    pad_up = np.tile(img_padded[0:1,:,:]*0 + padValue, (pad[0], 1, 1))\n",
    "    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n",
    "    pad_left = np.tile(img_padded[:,0:1,:]*0 + padValue, (1, pad[1], 1))\n",
    "    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n",
    "    pad_down = np.tile(img_padded[-2:-1,:,:]*0 + padValue, (pad[2], 1, 1))\n",
    "    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n",
    "    pad_right = np.tile(img_padded[:,-2:-1,:]*0 + padValue, (1, pad[3], 1))\n",
    "    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n",
    "\n",
    "    return img_padded, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"./Video_annotation/1.txt\", 'w')\n",
    "\n",
    "def demo_view(oriImg, input_heatmap, input_paf):\n",
    "    nms_joint = []\n",
    "    #input_heatmap = np.squeeze(input_heatmap)\n",
    "    heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 17))\n",
    "    paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 34))\n",
    "\n",
    "    \n",
    "    heatmap_avg = heatmap_avg + input_heatmap\n",
    "    paf_avg = paf_avg + input_paf\n",
    "    all_peaks = []\n",
    "    peak_counter = 0\n",
    "\n",
    "    for part in range(17-1):\n",
    "        map_ori = heatmap_avg[:,:,part]\n",
    "        map = gaussian_filter(map_ori, sigma=3)\n",
    "\n",
    "        map_left = np.zeros(map.shape)\n",
    "        #print(np.shape(map_left))\n",
    "        map_left[1:,:] = map[:-1,:]\n",
    "        map_right = np.zeros(map.shape)\n",
    "        map_right[:-1,:] = map[1:,:]\n",
    "        map_up = np.zeros(map.shape)\n",
    "        map_up[:,1:] = map[:,:-1]\n",
    "        map_down = np.zeros(map.shape)\n",
    "        map_down[:,:-1] = map[:,1:]\n",
    "\n",
    "        peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up,\\\n",
    "                                              map>=map_down, map > 0.1))\n",
    "        peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])) # note reverse\n",
    "        peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n",
    "        id = range(peak_counter, peak_counter + len(peaks))\n",
    "        peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n",
    "\n",
    "        all_peaks.append(peaks_with_score_and_id)\n",
    "        peak_counter += len(peaks)\n",
    "    \"\"\"connection_all = []\n",
    "    special_k = []\n",
    "    mid_num = 10###########################################################################################\n",
    "    #paf_avg = resized_vectormap\n",
    "    for k in range(len(mapIdx)):\n",
    "        score_mid = paf_avg[:,:,[x-17 for x in mapIdx[k]]]\n",
    "        #print(\"score_mid : \",np.shape(score_mid))\n",
    "        candA = all_peaks[limbSeq[k][0]]#all_peaks[limbSeq[k][0]-1]#157\n",
    "        candB = all_peaks[limbSeq[k][1]]#all_peaks[limbSeq[k][1]-1]#246\n",
    "        #candA : [(182, 68, 0.8127860426902771, 9)]\n",
    "        #candB :[(173, 92, 0.8721999526023865, 8)]\n",
    "        #print(\"candA, candB : \",candA, candB)\n",
    "        nA = len(candA)\n",
    "        nB = len(candB)\n",
    "        #print(\"nA, nB : \", nA, nB)\n",
    "        indexA, indexB = limbSeq[k]\n",
    "        #print(\"indexA, indexB : \", indexA, indexB)\n",
    "        if(nA != 0 and nB != 0):\n",
    "            connection_candidate = []\n",
    "            for i in range(nA):\n",
    "                for j in range(nB):\n",
    "                    vec = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                    norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n",
    "                    # failure case when 2 body parts overlaps\n",
    "                    if norm == 0:\n",
    "                        continue\n",
    "                    vec = np.divide(vec, norm)\n",
    "\n",
    "                    startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n",
    "                                   np.linspace(candA[i][1], candB[j][1], num=mid_num)))\n",
    "\n",
    "                    vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n",
    "                                      for I in range(len(startend))])\n",
    "\n",
    "                    vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n",
    "                                      for I in range(len(startend))])\n",
    "\n",
    "                    score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n",
    "                    score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n",
    "                    #print(\"score_midpts : \", score_midpts)\n",
    "                    criterion1 = len(np.nonzero(score_midpts > 0.0001)[0]) > 0.00005 * len(score_midpts)\n",
    "                    #print(\"score_with_dist_prior : \", score_with_dist_prior)\n",
    "                    criterion2 = score_with_dist_prior > 0.1\n",
    "                    #print(\"criterion1, criterion2 : \", criterion1, criterion2)\n",
    "                    if criterion1 and criterion2:\n",
    "                        #print(\"1\")\n",
    "                        connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n",
    "            #print(\"connection_candidate : \", connection_candidate)\n",
    "            connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n",
    "            connection = np.zeros((0,5))\n",
    "            #print(\"connection_candidate : \", connection_candidate)\n",
    "            for c in range(len(connection_candidate)):\n",
    "                i,j,s = connection_candidate[c][0:3]\n",
    "                if(i not in connection[:,3] and j not in connection[:,4]):\n",
    "                    connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n",
    "                    if(len(connection) >= min(nA, nB)):\n",
    "                        break\n",
    "            #print(\"connection : \", connection)\n",
    "            connection_all.append(connection)\n",
    "        else:\n",
    "            special_k.append(k)\n",
    "            connection_all.append([])\n",
    "    subset = -1 * np.ones((0, 20))######################################################\n",
    "    #print(\"subset1 : \", subset)\n",
    "    candidate = np.array([item for sublist in all_peaks for item in sublist])\n",
    "    #print(\"candidate : \", candidate)\n",
    "    #print(\"all_peaks : \", all_peaks)\n",
    "    for k in range(len(mapIdx)):\n",
    "        #print(\"--------------------------------------------\")\n",
    "        #print(\"k : \", k)\n",
    "        #print(\"special_k : \", special_k)\n",
    "        if k not in special_k:\n",
    "            #print(\"k is not in special_k : \",k)\n",
    "            partAs = connection_all[k][:,0]\n",
    "            partBs = connection_all[k][:,1]\n",
    "            indexA, indexB = np.array(limbSeq[k]) - 1\n",
    "\n",
    "            #print(\"connection_all : \",connection_all)\n",
    "            #print(\"connection_all[k] : \",connection_all[k])\n",
    "            for i in range(len(connection_all[k])): #= 1:size(temp,1)\n",
    "                #print(\"hello\")\n",
    "                found = 0\n",
    "                subset_idx = [-1, -1]\n",
    "                #print(\"subset's length :\", len(subset))\n",
    "                for j in range(len(subset)): #1:size(subset,1):\n",
    "                    #print(\"subset3 : \", subset)\n",
    "                    if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n",
    "                        subset_idx[found] = j\n",
    "                        found += 1\n",
    "                #print(\"found : \",found)\n",
    "                if found == 1:\n",
    "                    j = subset_idx[0]\n",
    "                    if(subset[j][indexB] != partBs[i]):\n",
    "                        subset[j][indexB] = partBs[i]\n",
    "                        subset[j][-1] += 1\n",
    "                        subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "                elif found == 2: # if found 2 and disjoint, merge them\n",
    "                    j1, j2 = subset_idx\n",
    "                    #print (\"found = 2\")\n",
    "                    membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n",
    "                    if len(np.nonzero(membership == 2)[0]) == 0: #merge\n",
    "                        subset[j1][:-2] += (subset[j2][:-2] + 1)\n",
    "                        subset[j1][-2:] += subset[j2][-2:]\n",
    "                        subset[j1][-2] += connection_all[k][i][2]\n",
    "                        subset = np.delete(subset, j2, 0)\n",
    "                    else: # as like found == 1\n",
    "                        subset[j1][indexB] = partBs[i]\n",
    "                        subset[j1][-1] += 1\n",
    "                        subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "\n",
    "                # if find no partA in the subset, create a new subset\n",
    "                elif not found and k < 17:\n",
    "                    row = -1 * np.ones(20)\n",
    "                    row[indexA] = partAs[i]\n",
    "                    row[indexB] = partBs[i]\n",
    "                    row[-1] = 2\n",
    "                    row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n",
    "                    subset = np.vstack([subset, row])\n",
    "    #print(\"end~\",subset)\n",
    "    # delete some rows of subset which has few parts occur\n",
    "    deleteIdx = [];\n",
    "    #print(subset)\n",
    "    for i in range(len(subset)):\n",
    "        #print(subset[i][-1])\n",
    "        if subset[i][-1] < 1 or subset[i][-2]/subset[i][-1] < 0.3:\n",
    "            deleteIdx.append(i)\n",
    "            #print(i)\n",
    "    subset = np.delete(subset, deleteIdx, axis=0)\n",
    "    #print(subset)\n",
    "    # visualize\n",
    "\n",
    "    cmap = matplotlib.cm.get_cmap('hsv')\n",
    "\n",
    "    canvas = oriImg#cv2.imread(oriImg) # B,G,R order\n",
    "    #canvas = canvas[:, :, (2,1,0)]\"\"\"\n",
    "    #print(all_peaks)########################################################################################\n",
    "    nms_joint.append(all_peaks)\n",
    "    #all_peaks_file = \",\".join(str(x) for x in all_peaks)\n",
    "    #f.write(all_peaks_file)\n",
    "    #f.write(\"\\n\")\n",
    "    \n",
    "    \"\"\"for i in range(16):\n",
    "        rgba = np.array(cmap(1 - i/16. - 1./32))\n",
    "        rgba[0:3] *= 255\n",
    "\n",
    "        for j in range(len(all_peaks[i])):\n",
    "            #print((all_peaks[i][j]))\n",
    "            #x = all_peaks[i][j][3]\n",
    "            #y = all_peaks[i][j][1]\n",
    "            cv2.circle(canvas, all_peaks[i][j][0:2], 2, colors[i], thickness=-1)\n",
    "            #cv2.circle(canvas, (x, y), 1, colors[i], thickness=-1)\n",
    "\n",
    "    to_plot = cv2.addWeighted(oriImg, 0.3, canvas, 0.7, 0)\n",
    "\n",
    "\n",
    "    # visualize 2# visual \n",
    "    stickwidth = 1\n",
    "\n",
    "    for i in range(15):\n",
    "        for n in range(len(subset)):\n",
    "            index = subset[n][np.array(limbSeq[i])-1]\n",
    "            #print(\"index : \", index)\n",
    "            if -1 in index:\n",
    "                continue\n",
    "            #print(\"hello\")\n",
    "            cur_canvas = canvas.copy()\n",
    "            Y = candidate[index.astype(int), 0]\n",
    "            X = candidate[index.astype(int), 1]\n",
    "            mX = np.mean(X)\n",
    "            mY = np.mean(Y)\n",
    "            length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n",
    "            angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n",
    "            polygon = cv2.ellipse2Poly((int(mY),int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n",
    "            cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n",
    "            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\"\"\"\n",
    "    #return canvas[:,:,[2,1,0]], nms_joint\n",
    "    return nms_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data load start\n",
      "test data load finish\n",
      "open pose init complete\n",
      "INFO:tensorflow:Restoring parameters from C:/Users/JAEKYU/Documents/Jupyter Notebook/Open_Pose/Weight/Weight.ckpt\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/101.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/102.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/103.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/104.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/105.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/106.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/108.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/115.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/117.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/120.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/124.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/125.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/128.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/132.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/134.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/135.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/138.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/139.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/140.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/141.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/143.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/148.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/149.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/15.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/150.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/18.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/19.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/23.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/24.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/25.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/26.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/27.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/28.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/29.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/36.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/42.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/43.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/46.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/47.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/48.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/49.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/50.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/51.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/54.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/55.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/56.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/57.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/58.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/59.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/60.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/62.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/66.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/68.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/69.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/7.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/71.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/72.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/74.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/77.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/78.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/79.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/80.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/85.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/87.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/92.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/93.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/94.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/95.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/96.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/97.avi\n",
      "./video_with_abstacle/TrainingSet_2/video_another_30/98.avi\n"
     ]
    }
   ],
   "source": [
    "limbSeq = [(9, 8), (8, 13), (8, 12), (8, 7), (12, 11), (11, 10), (13, 14), (14, 15), \\\n",
    "           (7, 6), (6, 3),  (3, 4),  (4, 5), (6, 2),   (2, 1),   (1, 0)]#16개 조인트, 15개 limb\n",
    "\n",
    "mapIdx = [(16, 17), (18, 19), (20, 21), (22, 23), (28, 29), (30, 31), (24, 25), (26, 27), (32, 33), \\\n",
    "          (34, 35), (38, 39), (40, 41), (36, 37), (42, 43), (44, 45)]#x, y vectormap Pairs 24,25\n",
    "\n",
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n",
    "img_width = 112\n",
    "video_data_path = \"./video_with_abstacle/TrainingSet_2/video_another_30/\"\n",
    "video_list = os.listdir(video_data_path)\n",
    "video_filename_list = os.listdir(video_data_path)\n",
    "for i, video in enumerate(video_list):\n",
    "    video_list[i] = video_data_path + video\n",
    "with tf.Session() as sess:\n",
    "#with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\n",
    "    obj = openpose(batch_size=1, sess = sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    SAVE_PATH = \"C:/Users/JAEKYU/Documents/Jupyter Notebook/Open_Pose/Weight/Weight.ckpt\"\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, SAVE_PATH)\n",
    "    for index, video in enumerate(video_list):\n",
    "        #cap = cv2.VideoCapture(0)\n",
    "        print(video)\n",
    "        cap = cv2.VideoCapture(video)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        #out = cv2.VideoWriter('output2.avi',fourcc, 30.0, (356,356))\n",
    "        #while(True):\n",
    "        sum_nms_joint = []\n",
    "        #print(\"frame start\")\n",
    "        for index_sub in range(60):\n",
    "            #print(\"caption read\")\n",
    "            ret, img_color = cap.read()\n",
    "            if(ret == True):\n",
    "                img_color = cv2.resize(img_color, (356, 356))\n",
    "                heatmap, vectormap = obj.demo_test([img_color])\n",
    "                \n",
    "                heatmap = np.squeeze(heatmap)\n",
    "                vectormap = np.squeeze(vectormap)\n",
    "                resized_heatmap = np.zeros((17, 356, 356))\n",
    "                resized_vectormap = np.zeros((34, 356, 356))\n",
    "                \n",
    "                heatmap = np.transpose(heatmap, (2,0, 1))\n",
    "                vectormap = np.transpose(vectormap, (2,0, 1))\n",
    "                \n",
    "                for k in range(17):\n",
    "                    resized_heatmap[k] = cv2.resize(heatmap[k], (356, 356))\n",
    "                for l in range(34):\n",
    "                    resized_vectormap[l] = cv2.resize(vectormap[l], (356, 356))\n",
    "\n",
    "                resized_heatmap = np.transpose(resized_heatmap, (1,2, 0))\n",
    "                resized_vectormap = np.transpose(resized_vectormap, (1,2, 0))\n",
    "\n",
    "                #output, nms_joint = demo_view(img_color, resized_heatmap, resized_vectormap)\n",
    "                nms_joint = demo_view(img_color, resized_heatmap, resized_vectormap)\n",
    "\n",
    "                sum_nms_joint.append(nms_joint)\n",
    "\n",
    "            if ret == False:\n",
    "                continue\n",
    "            #output = output[...,::-1]\n",
    "            #cv2.imshow('bgr', output)\n",
    "            #27 -> esc\n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                break\n",
    "        json_nms_joint = dict()\n",
    "        with open(\"./video_with_abstacle/TrainingSet_2/video_another_30_annotaion/\"+video_filename_list[index][0:-4]+'.json', 'w') as f:\n",
    "            #print(len(sum_nms_joint))\n",
    "            for i in range(len(sum_nms_joint)):\n",
    "                for j in range(16):\n",
    "\n",
    "                    if(len(sum_nms_joint[i][0][j]) == 0):\n",
    "                        pass\n",
    "                    else:\n",
    "                        #print(j, nms_joint[i][j][0][0:2])\n",
    "                        #print(type())\n",
    "                        json_nms_joint['%d'%j] = sum_nms_joint[i][0][j][0][0:2]\n",
    "                #json_nms_joint['%d'%j] = sum_nms_joint[i][j][0][0:2]\n",
    "                #print(json_nms_joint)\n",
    "                json_nms_joint\n",
    "                #json.\n",
    "                dumped = json.dumps(json_nms_joint, cls=NumpyEncoder)\n",
    "\n",
    "                json.dump(dumped, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "                json_nms_joint.clear()\n",
    "                #print(\"------------------------------------\")\n",
    "            f.close()\n",
    "\n",
    "        cap.release()\n",
    "        #out.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-68db55a84163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msum_nms_joint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#[image][0][part][몇 번째 사람][0:2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sum_nms_joint[0][0][0][0]#[image][0][part][몇 번째 사람][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_nms_joint[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_nms_joint = dict()\n",
    "with open('filename.json', 'w') as f:\n",
    "    for i in range(len(nms_joint)):\n",
    "        for j in range(16):\n",
    "            if(len(nms_joint[i][j]) == 0):\n",
    "                pass\n",
    "            else:\n",
    "                #print(j, nms_joint[i][j][0][0:2])\n",
    "                json_nms_joint['%d'%j] = nms_joint[i][j][0][0:2]\n",
    "        \n",
    "        print(json_nms_joint)\n",
    "        dumped = json.dumps(json_nms_joint, cls=NumpyEncoder)\n",
    "\n",
    "        json.dump(dumped, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "        json_nms_joint.clear()\n",
    "        print(\"------------------------------------\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_list = os.listdir(\"./video_with_abstacle/TrainingSet_2/video_another_30/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate (v_list):\n",
    "    v_list[i] = \"./video_with_abstacle/TrainingSet_2/video_another_30/\" + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in v_list:\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if(length != 30):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "    video_data = []\n",
    "    i = 0\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    while(i != length):\n",
    "        ret, frame = cap.read()\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (img_width, img_width))\n",
    "        video_data.append(frame)\n",
    "        i = i + 1\n",
    "    return joint_data, video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
